# Refactored Architecture Overview

## Phase 1 Dialogue System - Architecture

### Core Principles
✓ **Composability** — Each node is independent and testable  
✓ **Extensibility** — Clear places to add knowledge retrieval, fact lookup, etc.  
✓ **Maintainability** — Separated concerns, well-documented  
✓ **Debuggability** — State flows clearly through named nodes  

---

## Graph Pipeline

```
START
  ↓
load_npc_context        (Dialogue/nodes_context.py)
  ↓ Produces: system_prompt
  ↓
build_prompt            (Dialogue/nodes_prompt.py)
  ↓ Produces: full_prompt
  ↓
call_llm                (Dialogue/nodes_llm.py)
  ↓ Produces: raw_response
  ↓
format_response         (Dialogue/nodes_format.py)
  ↓ Produces: formatted_response
  ↓
END
```

---

## File Organization

### Core Graph Files
- **`Dialogue/state.py`** — `DialogueState` TypedDict (single source of truth for flow)
- **`Dialogue/dialogue_graph.py`** — Graph orchestration and entry point
- **`Dialogue/nodes_context.py`** — NPC context loading
- **`Dialogue/nodes_prompt.py`** — Prompt construction
- **`Dialogue/nodes_llm.py`** — LLM invocation
- **`Dialogue/nodes_format.py`** — Response formatting

### Supporting Files
- **`Dialogue/llm_provider.py`** — LLM API wrapper (centralizes API calls)
- **`Dialogue/npc.py`** — NPC character definition
- **`Dialogue/system_prompt.py`** — System prompt generation
- **`Dialogue/prompt_builder.py`** — (Future use for advanced prompt assembly)
- **`Dialogue/audit.py`** — (Future use for fact tracking & citations)

### Main & Config
- **`main.py`** — Interactive chatbot CLI
- **`config.py`** — API keys and configuration
- **`requirements.txt`** — Python dependencies

---

## State Flow Details

### DialogueState (Dialogue/state.py)

```python
class DialogueState(TypedDict):
    # Input context (set before graph execution)
    npc: NPC                          # Character to roleplay as
    user_input: str                   # Current user message
    conversation_history: str         # Previous exchanges
    
    # Intermediate data (built during execution)
    system_prompt: str                # NPC behavior instructions
    full_prompt: str                  # Complete LLM input
    
    # Output data (generated by graph)
    raw_response: str                 # Direct LLM output
    formatted_response: str           # Final response to user
```

---

## Extensibility Points for Future Phases

### Phase 2: Knowledge Retrieval
Add a `retrieve_knowledge` node between `build_prompt` and `call_llm`:

```python
# Dialogue/nodes_knowledge.py
def retrieve_knowledge(state: DialogueState) -> DialogueState:
    """
    Node: Lookup facts relevant to user's question.
    
    Future: Query world knowledge store for facts matching the user input.
    """
    user_input = state["user_input"]
    # TODO: Search world knowledge store
    # TODO: Add retrieval_results to state
    # TODO: Inject into prompt before calling LLM
    return state
```

Then update dialogue_graph.py:
```python
graph.add_node("retrieve_knowledge", retrieve_knowledge)
graph.add_edge("build_prompt", "retrieve_knowledge")
graph.add_edge("retrieve_knowledge", "call_llm")
```

### Phase 3: Fact Tracking & Audit Trails
Add to `DialogueState`:
```python
retrieval_results: Optional[List[Fact]]    # Facts matched to user query
facts_used: Optional[List[str]]            # Fact IDs used in response
audit_trail: Optional[List[Dict]]          # Full citation chain
```

Add a `track_facts` node before `format_response`:
```python
# Dialogue/nodes_audit.py
def track_facts(state: DialogueState) -> DialogueState:
    """Extract fact citations from LLM response."""
    # Parse response for fact references
    # Match against retrieval_results
    # Build audit_trail
    return state
```

### Phase 4: Error Handling & Routing
Add conditional edges for error cases:
```python
def route_after_llm(state: DialogueState):
    """Route based on LLM success/failure."""
    if "[Error" in state["raw_response"]:
        return "error_handler"
    return "format_response"

graph.add_conditional_edges(
    "call_llm",
    route_after_llm,
    {"error_handler": "error_handler", "format_response": "format_response"}
)
```

---

## Using the System

### Run Interactive Session
```bash
python main.py
```

### Programmatic Usage
```python
from Dialogue.npc import NPC
from Dialogue.dialogue_graph import create_dialogue_graph, run_dialogue_turn

npc = NPC(name="Aldric", ...)
graph = create_dialogue_graph()

response = run_dialogue_turn(graph, npc, "Hello!", "")
print(response)
```

### Test a Single Turn
```bash
python test_refactor.py
```

---

## Design Rationale

### Why Separate Node Files?
Each `nodes_*.py` file contains related nodes for a specific concern:
- Easy to find code by responsibility
- Simple to test nodes independently
- Clear where to add new functionality

### Why State TypedDict?
- Single source of truth for what data flows through graph
- IDE autocomplete support
- Self-documenting code
- Easy to evolve for new phases

### Why LLMProvider?
- Centralizes API calls and error handling
- Easy to swap implementations (Vertex, Claude, local, etc.)
- Testable in isolation
- Future: can add caching, rate limiting, etc.

### Why Stateless Graph?
- `create_dialogue_graph()` called once, reused for many turns
- State is passed in, not stored
- Enables concurrent dialogue sessions
- Matches LangGraph patterns

---

## Next Steps (Phase 2)

1. **Build World Knowledge Store** — `World/store.py` with entities, facts, events
2. **Add Knowledge Retrieval Node** — Search store for relevant facts
3. **Update System Prompt** — Tell NPC to cite facts from world
4. **Implement Audit Tracking** — Record which facts were used
5. **Add Visualization** — Debug tool to inspect fact lookup results
